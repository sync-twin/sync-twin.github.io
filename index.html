<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Manipulation - Ruopeng Huang, Boyu Yang, Wenlong Gui, Jeremy Morgan, Erdem Biyik, Jiachen Li">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="A digital twin framework for fast 3D reconstruction and real-time synchronization enabling safe robotic manipulation in dynamic, occluded environments.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="robotics, hierarchical learning, vision-language-action models, demonstration decomposition, retrieval-based learning, visuomotor policy, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Ruopeng Huang, Boyu Yang, Wenlong Gui, Jeremy Morgan, Erdem Biyik, Jiachen Li">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Manipulation">
  <meta name="citation_author" content="Huang, Ruopeng">
  <meta name="citation_author" content="Yang, Boyu">
  <meta name="citation_author" content="Gui, Wenlong">
  <meta name="citation_author" content="Morgan, Jeremy">
  <meta name="citation_author" content="Biyik, Erdem">
  <meta name="citation_author" content="Li, Jiachen">
   <!-- <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="Robotics: Science and Systems (RSS)">-->
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Manipulation - Ruopeng Huang, Boyu Yang, Wenlong Gui, Jeremy Morgan, Erdem Biyik, Jiachen Li | Academic Research</title>
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/tasl.ico">
  <link rel="apple-touch-icon" href="static/images/tasl.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Manipulation",
    "description": "A digital twin framework for fast 3D reconstruction and real-time synchronization enabling safe robotic manipulation in dynamic, occluded environments.",
    "author": [
      {
        "@type": "Person",
        "name": "Ruopeng Huang",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Southern California"
        }
      },
      {
        "@type": "Person",
        "name": "Boyu Yang",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Southern California"
        }
      },
      {
        "@type": "Person",
        "name": "Wenlong Gui",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Southern California"
        }
      },
      {
        "@type": "Person",
        "name": "Jeremy Morgan",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Southern California"
        }
      },
      {
        "@type": "Person",
        "name": "Erdem Biyik",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Southern California"
        }
      },
      {
        "@type": "Person",
        "name": "Jiachen Li",
        "affiliation": {
          "@type": "Organization",
          "name": "University of California, Riverside"
        }
      }
    ],
    "datePublished": "2026-01-01",
    "publisher": {
      "@type": "Organization",
      "name": ""
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["robotics", "hierarchical learning", "vision-language-action models", "demonstration decomposition", "machine learning", "computer vision"],
    "abstract": "To enable robots to achieve long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks typically employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. To finetune the VLM planner and let it learn to decompose the target task, a few human demonstrations are segmented into sub-tasks by either human annotation or heuristic rules. The heuristic sub-tasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (SyncTwin) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of decomposed sub-task intervals with the training data of low-level visuomotor policies to fully exploit its capability.",
    "citation": "@inproceedings{huang2026SyncTwin, title={SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Manipulation}, author={Huang, Ruopeng and Yang, Boyu and Gui, Wenlong and Morgan, Jeremy and Biyik, Erdem and Li, Jiachen}, booktitle={Proceedings of Robotics: Science and Systems (RSS)}, year={2026}}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "University of California, Riverside",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/tasl.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Link -->
  <div style="position: fixed; top: 20px; right: 20px; z-index: 1000;">
    <a href="https://tasl.ucr.edu/publications/" target="_blank"
       class="external-link button is-normal is-rounded is-primary"
       style="font-weight: 500;">
      <span class="icon">
        <i class="fas fa-flask"></i>
      </span>
      <span>More Works</span>
    </a>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column is-12 has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-2 publication-title">SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Manipulation<br><span class="is-size-4" style="font-style: italic; font-weight: 400; color: #666; font-family: 'Inter', sans-serif;"></span></h1>
            <!--<div class="is-size-5" style="margin-top: 15px; margin-bottom: 20px;">
              <span style="font-weight: 600;">Robotics: Science and Systems (RSS) 2026</span>
            </div> -->
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://huangruopeng.github.io/" target="_blank">Ruopeng Huang</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Boyu Yang</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Wenlong Gui</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="http://jeremymorgan.io/" target="_blank">Jeremy Morgan</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://ebiyik.github.io/" target="_blank">Erdem Biyik</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://jiachenli94.github.io/" target="_blank">Jiachen Li</a><sup>1*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of California, Riverside &nbsp;&nbsp; <sup>2</sup>University of Southern California</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author</small></span>
                    <br>
                    <span class="author-block" style="display: flex; align-items: center; justify-content: center; gap: 8px;">
                      <img src="static/images/tasl.svg" alt="TASL Lab Logo" style="height: 20px; width: auto;">
                      <a href="https://tasl.ucr.edu/" target="_blank" style="font-weight: 500; color: #2563eb; text-decoration: underline;">
                        Trustworthy Autonomous Systems Laboratory (TASL)
                      </a>
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2601.09920"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <span class="link-block">
                    <a href="#"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- 1. Hero (主图 + general 简介) -->
<section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <!-- 主图（保留你的 qualitative 图） -->
      <img
      src="static/images/overview.png"
      alt="Overview of SyncTwin framework"
      style="
        display: block;
        margin: 0 auto 18px auto;
        max-width: 900px;
        width: 80%;
        height: auto;
      "
    >

      <!-- General 简介（一句话概括） -->
      
      <p class="subtitle has-text-justified" style="max-width:900px; margin: 0 auto 18px auto;">
       We will introduce SyncTwin, a novel framework for fast digital-twin construction and real-time synchronization designed for safe robotic manipulation in dynamic, occluded environments. We develop fast RGB-only reconstruction, real-time tracking, and MPC into a synchronized digital twin for safe execution in dynamic, occluded environments. 
      </p>
    </div>
  </div>
</section>

<!-- 2. YouTube 视频 -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <h2 class="title is-4 has-text-centered">YouTube</h2>
        <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.08); margin-bottom: 18px;">
          <!-- 替换下面的 video id -->
          <iframe
            style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border-radius: 8px;"
            src="https://www.youtube.com/embed/vbVTZYMYg38"
            frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen>
          </iframe>
        </div>
        <p class="has-text-centered" style="color:#666; font-size:0.95rem;">video demo</p>
      </div>
    </div>
  </div>
</section>

<!-- 3. Abstract（保留你的 Abstract） -->
<section class="section hero is-light">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
         <div id="motivation-media" style="text-align:center; margin-bottom: 20px;">
        <video
          autoplay
          loop
          muted
          playsinline
          preload="metadata"
          poster="static/images/intro_poster.png"
          style="max-width: 900px; width: 80%; height: auto;
                border-radius: 8px;
                box-shadow: 0 4px 16px rgba(0,0,0,0.1);"
        >
          <source src="static/videos/intro.mp4" type="video/mp4">
        </video>
      </div>

        <div class="content has-text-justified" style="max-width:900px; margin: 0 auto;">

          <p>
            Accurate and safe robotic manipulation under dynamic and visually occluded conditions remains a core challenge in real-world deployment. We introduce SyncTwin, a novel digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware robotic manipulation in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. The synchronized twin enables motion planners to compute collision-free and dynamically feasible trajectories in simulation, which are safely executed on the real robot through a closed real-to-sim-to-real loop. Experiments in dynamic and occluded scenes show that SyncTwin improves manipulation performance and motion safety, demonstrating the effectiveness of digital twin synchronization for real-world robotic execution.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 4. Motivation -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Motivation</h2>

     

        <div id="motivation-media" style="text-align:center; margin-bottom: 20px;">
        <video
          autoplay
          loop
          muted
          playsinline
          preload="metadata"
          poster="static/images/intro_poster.png"
          style="max-width: 900px; width: 50%; height: auto;
                border-radius: 8px;
                box-shadow: 0 4px 16px rgba(0,0,0,0.1);"
        >
          <source src="static/videos/motivation.mp4" type="video/mp4">
        </video>
      </div>


        <!-- Motivation 文案（已用你之前 method overview 中的文字，精简并保留关键点） -->
        <div class="content has-text-justified" style="max-width:900px; margin: 0 auto;">
          <p>
           Real-world safe manipulation is challenged by partial single-view perception and dynamic scenes with moving objects and occlusions, causing planners to operate on incomplete or outdated geometry and resulting in unsafe execution.
          </p>
          <p>
           To overcome these limitations, a robot needs a continuously synchronized digital twin, one that mirrors the real world in real time and provides accurate geometry for planning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 5. Method（Stage1 / Stage2，gif + 文字） -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Method</h2>

        <!-- 方法概览图（保留你的 teaser/method 图） -->
        <img
        src="static/images/method.png"
        alt="SyncTwin Method"
        style="
          display: block;
          margin: 0 auto 18px auto;
          max-width: 900px;
          width: 80%;
          height: auto;
        "
      >

        <!-- 高层说明 -->
        <div class="content has-text-justified" style="max-width:900px; margin: 0 auto 12px auto;">
          <p>
            The full SyncTwin method consists of two stages. 
            Stage I performs fast digital-twin construction. generates scene-level point clouds from RGB inputs. Object-level point clouds are extracted via projection, segmentation, and denoising, then converted into lightweight meshes and stored in a memory bank.Stage II performs online synchronization. A RealSense provides live RGB-D frames. After segmentation, partial point clouds are aligned to the memory-bank assets using colored ICP, and the updated poses are sent to Isaac Sim for MPC planning, closing the loop.
          </p>
        </div>

        <!-- Stage 1 -->
        <h3 class="title is-4 has-text-centered">Stage I — Fast Digital Twin Construction</h3>
       <div id="motivation-media" style="text-align:center; margin-bottom: 20px;">
        <video
          autoplay
          loop
          muted
          playsinline
          preload="metadata"
          poster="static/images/intro_poster.png"
          style="max-width: 900px; width: 80%; height: auto;
                border-radius: 8px;
                box-shadow: 0 4px 16px rgba(0,0,0,0.1);"
        >
          <source src="static/videos/method-stage1.mp4" type="video/mp4">
        </video>
      </div>

        <div class="content has-text-justified" style="max-width:900px; margin: 0 auto 14px auto;">
          <p>
            Here is a closer look at Stage I. VGGT generates a scene point cloud, then we detect projection intersections and segment object regions, but due to prediction error, the result still contains noise.
            To remove this noise, we introduce a geometric sphere-expansion method that identifies opening edges and cleanly separates the supporting plane.
            This produces clean point clouds, which we downsample, centralize, and finally convert into lightweight meshes stored in the memory bank.
          </p>
        </div>

        <!-- Stage 2 -->
        <h3 class="title is-4 has-text-centered">Stage II — Online Digital Twin Synchronization  </h3>
       
        <div id="motivation-media" style="text-align:center; margin-bottom: 20px;">
        <video
          autoplay
          loop
          muted
          playsinline
          preload="metadata"
          style="max-width: 900px; width: 80%; height: auto;
                border-radius: 8px;
                box-shadow: 0 4px 16px rgba(0,0,0,0.1);"
        >
          <source src="static/videos/method-stage2.mp4" type="video/mp4">
        </video>
      </div>

        <div class="content has-text-justified" style="max-width:900px; margin: 0 auto 6px auto;">
          <p>
            Stage II keeps the digital twin synchronized. We use a sliding-window memory to ensure temporally stable segmentation: the first frame saves an initial prompt, and subsequent frames update the memory representation.
            Each incoming frame is encoded together with the memory features and decoded by SAM, producing consistent masks even under occlusions. The resulting masks support real-time object tracking.
            The updated object states drive the MPC planner inside Isaac Sim, forming a continuous real-to-sim-to-real loop.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- 6. Experiments / Results -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Experiments & Results</h2>

        <!-- EXPERIMENT 1 -->
        <div style="margin-top: 18px;">
          <h3 class="title is-4 has-text-centered">Experiment  — Fast 3D Assets Reconstruction</h3>

      

          <!-- Image -->
          <div id="exp1-img" style="text-align:center; margin-bottom: 12px;">
            <img src="static/images/exp-assets-construction.png" alt="Fast 3D reconstruction results" style="max-width: 900px; width: 80%; height: auto; border-radius:8px; box-shadow:0 4px 16px rgba(0,0,0,0.1);">
          </div>

          <!-- Description / Metrics -->
          <div class="content has-text-justified" style="max-width:900px; margin: 0 auto 18px auto;">
     
            <p>
            We evaluate our fast 3D asset reconstruction across multiple baselines. As shown on the left, SyncTwin produces clean object geometry using only five to ten frames, while traditional methods such as Photogrammetry, 3D Gaussian Splatting, and Nerfstudio at least require 20 to 60 frames.
            SyncTwin reconstructs mesh in 1 to 2 minutes, significantly faster than all baselines.
            And our method preserves fine-grained geometric details (e.g., bear's ear shape remains well )

            </p>
               <!-- Video -->
            <div id="exp1-video" style="text-align:center; margin-bottom: 12px;">
              <video
                autoplay
                loop
                muted
                playsinline
                preload="metadata"
                style="max-width: 900px; width: 80%; height: auto; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.1);"
              >
                <source src="static/videos/exp-assets-construction.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

        <hr>

        <!-- EXPERIMENT 2 -->
        <div style="margin-top: 18px;">
          <h3 class="title is-4 has-text-centered">Experiment — Avoid Collision in the Real World</h3>

          <!-- Two videos side-by-side OR stacked on mobile -->
          <div class="two-gif-row" style="margin-bottom: 12px;">
            <div id="exp2-video1" class="gif-placeholder" style="padding: 0; border: none; box-shadow:none; display:flex; align-items:center; justify-content:center;">
              <video
                autoplay
                loop
                muted
                playsinline
                preload="metadata"
                style="max-width: 100%; width: 80%; height: auto; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.1);"
              >
                <source src="static/videos/exp-avoid-collision1.mp4" type="video/mp4">
              </video>

             
            
      
            </div>

               <!-- Description / Metrics -->
          <div class="content has-text-justified" style="max-width:900px; margin: 0 auto 18px auto;">      
             <p>
              We next evaluate dynamic obstacle avoidance in real-world scenarios. On the left, using NVBlox, the robot often fails to avoid collisions because voxel-based geometry is incomplete and updates slowly. On the right, SyncTwin maintains accurate object geometry through real-time alignment, enabling the robot to safely avoid obstacles, even when they move during execution.
            </p>
           
          </div>

          

            <div id="exp2-video2" class="gif-placeholder" style="padding: 0; border: none; box-shadow:none; display:flex; align-items:center; justify-content:center;">
              <video
                autoplay
                loop
                muted
                playsinline
                preload="metadata"
                style="max-width: 100%; width: 80%; height: auto; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.1);"
              >
                <source src="static/videos/exp-avoid-collision2.mp4" type="video/mp4">
              </video>
            </div>
          </div>

          <!-- Supporting Image -->
          <div id="exp2-img" style="text-align:center; margin-bottom: 12px;">
            <img src="static/images/exp-avoid-collision-result.png" alt="Avoiding collisions in real-world scenarios" style="max-width: 720px; width: 80%; height: auto; border-radius:8px; box-shadow:0 4px 16px rgba(0,0,0,0.1);">
          </div>

          <!-- Description / Metrics -->
          <div class="content has-text-justified" style="max-width:900px; margin: 0 auto 18px auto;">      
             <p>
             Quantitatively, SyncTwin substantially outperforms NVBlox. For unseen obstacles, we achieve up to 85.5% success in the self-rotation condition, and 71.5% in the enter-trajectory condition, compared to NVBlox at 50.3% and 37.0%. For seen objects stored in the memory bank, having full object geometry leads to even stronger results: SyncTwin reaches 93.5% and 78.8% success, respectively. These results validate the importance of using accurate, asset-level geometry for safety-critical planning.

            </p>
           
          </div>
        </div>

        <hr>

        <!-- EXPERIMENT 3 -->
        <div style="margin-top: 18px;">
          <h3 class="title is-4 has-text-centered">Experiment — Safe Grasping Under Single-View Occlusion</h3>

          <!-- Video -->
          <div id="exp3-video" style="text-align:center; margin-bottom: 12px;">
            <video
              autoplay
              loop
              muted
              playsinline
              preload="metadata"
              style="max-width: 900px; width: 80%; height: auto; border-radius: 8px; box-shadow: 0 4px 16px rgba(0,0,0,0.1);"
            >
              <source src="static/videos/exp-safe-grasp.mp4" type="video/mp4">
            </video>
          </div>

          <!-- Image -->
          <div id="exp3-img" style="text-align:center; margin-bottom: 12px;">
            <img src="static/images/exp-safe-grasp-result.png" alt="Safe grasping under occlusion" style="max-width: 900px; width: 60%; height: auto; border-radius:8px; box-shadow:0 4px 16px rgba(0,0,0,0.1);">
          </div>

          <!-- Description / Metrics -->
          <div class="content has-text-justified" style="max-width:900px; margin: 0 auto 18px auto;">
            <p>
            Finally, we demonstrate safe grasping under single-view occlusion. Without geometry completion, the robot sees only partial point clouds and often generates unsafe or failing grasps, especially for asymmetric objects such as cups with handles. With SyncTwin, the complete 3D asset from the memory bank replaces the partial observation, leading to more stable grasp candidates and significantly higher success rates. As shown in the table, performance improves by over 20% on challenging objects, enabling reliable grasping in real-world scenes.

            </p>
           
          </div>
        </div>

      </div>
    </div>
  </div>
</section>









<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{huang2026synctwin,
  title={SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping},
  author={Huang, Ruopeng and Yang, Boyu and Gui, Wenlong and Morgan, Jeremy and Biyik, Erdem and Li, Jiachen},
  journal={arXiv preprint arXiv:2601.09920},
  year={2026}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->




  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
